1. **Initial investigation: ignoring nonlinearity (for now)**
    a. Use ordinary least squares (OLS) by using the `lm` engine and LASSO (`glmnet` engine) to build  a series of initial regression models for your quantitative outcome as a function of the predictors of interest. (As part of data cleaning, exclude any variables that you don't want to consider as predictors.)
        - You'll need two model specifications, `lm_spec` and `lm_lasso_spec` (you'll need to tune this one).
    b. For each set of variables, you'll need a `recipe` with the `formula`, `data`, and pre-processing steps
        - You may want to have steps in your recipe that remove variables with near zero variance (`step_nzv()`), remove variables that are highly correlated with other variables (`step_corr()`), normalize all quantitative predictors (`step_normalize(all_numeric_predictors())`) and add indicator variables for any categorical variables (`step_dummy(all_nominal_predictors())`).
        - These models should not include any transformations to deal with nonlinearity. You'll explore this in the next investigation.
    c. Estimate the test performance of the models using CV. Report and interpret (with units) the CV metric estimates along with a measure of uncertainty in the estimate (`std_error` is readily available when you used `collect_metrics(summarize=TRUE)`).
        - Compare estimated test performance across the models. Which models(s) might you prefer?
    d. Use residual plots to evaluate whether some quantitative predictors might be better modeled with nonlinear relationships.
    e. Which variables do you think are the most important predictors of your quantitative outcome? Justify your answer. Do the methods you've applied reach consensus on which variables are most important? What insights are expected? Surprising?
        - Note that if some (but not all) of the indicator terms for a categorical predictor are selected in the final models, the whole predictor should be treated as selected.

<br>
a & b. 
```{r}
library(dplyr)
library(ggplot2)
library(readr)
library(lubridate)
library(astsa)
library(splines)
library(tidymodels)
tidymodels_prefer()
```

```{r}
health1 <- read.csv("heart_failure_clinical_records_dataset.csv")
```

```{r}
set.seed(321)

data_split <- initial_split(health1, prop = 0.75,strata = creatinine_phosphokinase) #Create Train/Test set
health_train <- training(data_split) # Fit model to this
health_test  <- testing(data_split)
health_cv13 <- vfold_cv(health_train, v = 13, strata = creatinine_phosphokinase)
```

```{r}
# model spec
lm_spec <- 
    linear_reg() %>% 
    set_engine(engine = 'lm') %>% 
    set_mode('regression') 

lm_lasso_spec_tune <- 
  linear_reg() %>%
  set_args(mixture = 1, penalty = tune()) %>%
  set_engine(engine = 'glmnet') %>%
  set_mode('regression') 
```

```{r}
# recipes & workflows

##  non-lasso recipe + wf 
life_rec<-recipe(creatinine_phosphokinase ~ ., data = health_train) %>%
  step_nzv(all_numeric_predictors()) %>%
  step_corr(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors())

training_prep <- life_rec %>%
  prep() %>%
  juice()

lm_model_wf <- workflow() %>%
  add_recipe(life_rec) %>% 
  add_model(lm_spec)

lm_fit_train <- lm_model_wf %>%
  fit(data=health_train)

training_prep %>%
  select(creatinine_phosphokinase) %>%
  bind_cols( predict(lm_fit_train, health_train)) %>%
  metrics(estimate = .pred, truth = creatinine_phosphokinase)

lm_fit_train %>%
  tidy()

lm_fit_cv <- fit_resamples(lm_model_wf, resamples =health_cv13, metrics = metric_set(rmse, mae, rsq))

lm_fit_cv %>% 
  collect_metrics()

lm_fit_test <- last_fit(lm_model_wf,
         split = data_split) 

lm_fit_test %>%
  collect_metrics() #Evaluation on Test Data
```

```{r}
# lasso wf 
lasso_wf_tune <- workflow() %>% 
  add_recipe(life_rec) %>% # recipe defined above
  add_model(lm_lasso_spec_tune)

penalty_grid <- grid_regular(
  penalty(range = c(-5, 3)), #log10 transformed 10^-5 to 10^3
  levels = 50)

# tune lasso 
penalty_grid <- grid_regular(
  penalty(range = c(-3, 1)), #log10 transformed 
  levels = 30)

tune_output <- tune_grid( # new function for tuning parameters
  lasso_wf_tune, # workflow
  resamples = health_cv13, # cv folds
  metrics = metric_set(rmse, mae),
  grid = penalty_grid) 
  
best_penalty <- select_best(tune_output, metric = 'mae') # choose penalty value based on lowest cv mae
best_penalty

best_se_penalty <- select_by_one_std_err(tune_output, metric = 'mae', desc(penalty)) # choose largest penalty value within 1 se of the lowest cv mae
best_se_penalty

final_wf <- finalize_workflow(lasso_wf_tune, best_penalty) # incorporates penalty value to workflow
final_wf_se <- finalize_workflow(lasso_wf_tune, best_se_penalty) # incorporates penalty value to workflow

final_fit <- fit(final_wf, data = health_train)
final_fit_se <- fit(final_wf_se, data = health_train)

tidy(final_fit)
tidy(final_fit_se)

```
c.

```{r}
mod_ols <- fit_resamples(lm_model_wf, 
      resamples = lifeexpectancy_cv7, 
      metrics = metric_set(rmse, rsq, mae)
      )  %>%
      collect_metrics(summarize = TRUE)

mod_lasso <- fit_resamples(life_final_wf, 
      resamples = health_cv13, 
      metrics = metric_set(rmse, rsq, mae)
      )  %>%
      collect_metrics(summarize = TRUE)
```

d.

```{r}
# visual residuals
#  Residuals vs. predictions
ggplot(mod1_output, aes(x = .pred, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()
# Residuals vs. predictors (x's)
 
ggplot(mod1_output, aes(x = height, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()
```
e.

<br>

2. **Summarize investigations**
    - Decide on an overall best model based on your investigations so far. To do this, make clear your analysis goals. Predictive accuracy? Interpretability? A combination of both?

Our analysis is more focused on interpretability than predictive accuracy as we are interested in which of our predictor variables have the greatest impact on a person's CPK (creatinine phosphokinase) levels rather than, for example, trying to predict a person's CPK levels based on other health indicators. High CPK levels usually indicate that there has been injury or stress to muscle tissue, the heart, or the brain. We want to see which health indicators have the strongest influence on damage to the body. We do not care to predict the CPK levels themselves as that does not carry much meaning for us.

<br>

3. **Societal impact**
    - Are there any harms that may come from your analyses and/or how the data were collected?
    - What cautions do you want to keep in mind when communicating your work?

We are using various health indicators to predict another health indicator which can be convoluted. For example, we are using death event to predict CPK levels when the CPK levels were measured before the follow up period in which they may have died. We could get rid of death event as a predictor to fix this. 

Harms can come from our analyses if we try to generalize the models too broadly. For example, since all of the patients were over 40, this information may not be useful when looking at a young person with heart failure. While this is rare, it is still important. Additionally, this sample is from patients from a hospital in Pakistan. We do not know what environmental factors may be at play here in this specific location. We also want to be careful not to generalize because the sample is pretty limited (only 299 patients). 


<br><br><br>
