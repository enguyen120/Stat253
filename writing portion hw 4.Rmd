---
title: "writing portions hw4"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Start working on building a classification model to answer a research question on your data set. For HW4, only include your classification model work (leave your regression models work in another file).

Specify the research question for a classification task.

Try to implement at least 2 different classification methods to answer your research question.

Reflect on the information gained from these two methods and how you might justify this method to others.

Keep in mind that the final project will require you to complete the pieces below. Use this as a guide for your work but don’t try to accomplish everything for HW4:

# Classification - Methods
Indicate at least 2 different methods used to answer your classification research question.

The question we are trying to answer is, what predictors influence the probability of a death event? We are using logistic regression and decision trees to answer this question.  

Describe what you did to evaluate the models explored.

To evaluate the models, we used cross validation and the out of bag error. For cross validation, we randomly  split a data set into two groups that are called folds, given by k. For each fold, the data is divided randomly into a training set  and test set, k-1 folds. The training data is used to train the model to make predictions about the data and the test set is used to see how well the model works on data it has not seen before, allowing us to test its accuracy. The resulting metrics from cross validation are averaged over the number of folds to get a result that encapsulates all the folds and tests that were done. For the out of bag error, the algorithm uses bootstrapping which is where the algorithm randomly resamples the dataset to generate the model. For the out of bag error, the algorithm randomly leaves out some cases out of the resampling and uses these left out cases to test the model and determine the accuracy of the decision trees. 

Indicate how you estimated quantitative evaluation metrics.

We estimated all of our quantitative evaluation metrics using cross validation. 

Describe the goals / purpose of the methods used in the overall context of your research investigations.

Our data set includes data on different health factors that influence the probability of a person dying from heart failure. It makes logical sense that the classification models that we are creating for this section of the project will generate the most valuable results from the data set and will be the most important in regards to the overall context of our research investigations. 

# Classification - Results
Summarize your final model and justify your model choice (see below for ways to justify your choice).
Compare the different classification models tried in light of evaluation metrics, variable importance, and data context.
Display evaluation metrics for different models in a clean, organized way. This display should include both the estimated metric as well as its standard deviation. (This won’t be available from OOB error estimation. If using OOB, don’t worry about reporting the SD.)
Broadly summarize conclusions from looking at these evaluation metrics and their measures of uncertainty.
# Classification - Conclusions 
Interpret evaluation metric(s) for the final model in context. Does the model show an acceptable amount of error? - If using OOB error estimation, display the test (OOB) confusion matrix, and use it to interpret the strengths and weaknesses of the final model. - Summarization should show evidence of acknowledging the data context in thinking about the sensibility of these results.