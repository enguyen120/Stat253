```{r}
library(dplyr)
library(ggplot2)
library(readr)
library(lubridate)
library(splines)
library(tidymodels)
library(gridExtra)
library(vip)
library(probably)
tidymodels_prefer()
conflicted::conflict_prefer("vi", "vip") 

```

```{r}
health1 <- read.csv("heart_failure_clinical_records_dataset.csv")
```

```{r}
health1 <- health1%>%
  mutate(creatinine_phosphokinase = log(creatinine_phosphokinase)) %>%
  mutate(platelets = log(platelets))
  
```


Start working on building a classification model to answer a research question on your data set. For HW4, only include your classification model work (leave your regression models work in another file).

For this homework,

Specify the research question for a classification task.

Our research question is: What predictors best predict death (DEATH_EVENT)?

Try to implement at least 2 different classification methods to answer your research question.

Reflect on the information gained from these two methods and how you might justify this method to others.

Keep in mind that the final project will require you to complete the pieces below. Use this as a guide for your work but don’t try to accomplish everything for HW4:

Classification - Methods
Indicate at least 2 different methods used to answer your classification research question.

Describe what you did to evaluate the models explored.

To evaluate the models, we used cross validation and the out of bag error. For cross validation, we randomly  split a data set into two groups that are called folds, given by k. For each fold, the data is divided randomly into a training set  and test set, k-1 folds. The training data is used to train the model to make predictions about the data and the test set is used to see how well the model works on data it has not seen before, allowing us to test its accuracy. The resulting metrics from cross validation are averaged over the number of folds to get a result that encapsulates all the folds and tests that were done. For the out of bag error, the algorithm uses bootstrapping which is where the algorithm randomly resamples the dataset to generate the model. For the out of bag error, the algorithm randomly leaves out some cases out of the resampling and uses these left out cases to test the model and determine the accuracy of the decision trees.

Indicate how you estimated quantitative evaluation metrics.

We estimated all of our quantitative evaluation metrics using cross validation and the out of bag error. 

Describe the goals / purpose of the methods used in the overall context of your research investigations.

Our data set includes data on different health factors that influence the probability of a person dying from heart failure. It makes logical sense that the classification models that we are creating for this section of the project will generate the most valuable results from the data set and will be the most important in regards to the overall context of our research investigations.

Classification - Results

Summarize your final model and justify your model choice (see below for ways to justify your choice).

Compare the different classification models tried in light of evaluation metrics, variable importance, and data context.
Display evaluation metrics for different models in a clean, organized way. This display should include both the estimated metric as well as its standard deviation. (This won’t be available from OOB error estimation. If using OOB, don’t worry about reporting the SD.)

In our final model we predicted the probability of someone dying using logistic regression. We chose decision trees over logistic regression because it has a higher likelihood it will correctly predict death at an accuracy rate of 85.61%; compared to our logistic regression model which has an accuracy of 82.27% on predicting death event. Since the sensitivity for the logistic regression is low, it is worse at predicting whether or not they will die, which is another reason why we've chosen the decision trees. Although the ROC AUC curve for logistic regression is 88%, it is still not the same as the actual prediction of the death event, which is lower than logisitc regression.

Broadly summarize conclusions from looking at these evaluation metrics and their measures of uncertainty.
Classification - Conclusions - Interpret evaluation metric(s) for the final model in context. Does the model show an acceptable amount of error? - If using OOB error estimation, display the test (OOB) confusion matrix, and use it to interpret the strengths and weaknesses of the final model. - Summarization should show evidence of acknowledging the data context in thinking about the sensibility of these results.

The model does not show an acceptable amount of error because if was told that the prediction of a death event (or not dying) was 85% correct, it is quite low. In our OOB confusion matrix, there were 187 people whose that were predicted to not happen and did not happen, 26 people whose deaths we predicted to not happen but happened, 17 people who we predicted to die and did not die and 70 people who we predicted to die who died. Ideally, the model would have as close to 100% accuracy to predict a death event, furthermore we would prefer the model to predict that someone was going to die and they don't die versus the opposite. Given the context of our data, our results are not sensible and our model should not be used to predict death event of these heart failure patients. 

Decision trees

```{r}

health1$DEATH_EVENT <- as.factor(health1$DEATH_EVENT)

#Model Specification
rf_spec <- rand_forest() %>%
  set_engine(engine = 'ranger') %>% 
  set_args( floor(sqrt(13)),
           trees = 1000, 
           min_n = 2,
           probability = FALSE, # FALSE: get hard predictions
           importance = 'impurity') %>%
  set_mode('classification') # change this for regression

# Recipe
data_rec <- recipe(DEATH_EVENT ~ ., data = health1) 

# Workflows
data_wf <- workflow() %>%
  add_model(rf_spec ) %>%
  add_recipe(data_rec)

```

```{r}
set.seed(123) 
data_fit <- fit(data_wf, data = health1)
```

```{r}
# Custom Function to get OOB predictions, true observed outcomes and add a user-provided model label
rf_OOB_output <- function(fit_model, model_label, truth){
    tibble(
          .pred_class = fit_model %>% extract_fit_engine() %>% pluck('predictions'), #OOB predictions
          class = truth,
          label = model_label
      )
}


rf_OOB_output(data_fit, sqrt(13), health1 %>% pull(DEATH_EVENT))
```

```{r}
#Evaluate OOB Metrics
data_rf_OOB_output <- bind_rows(
    rf_OOB_output(data_fit, sqrt(13), health1 %>% pull(DEATH_EVENT)))

data_rf_OOB_output %>% 
    group_by(label) %>%
    accuracy(truth = class, estimate = .pred_class)
```

```{r}
rf_OOB_output(data_fit,12, health1 %>% pull(DEATH_EVENT)) %>%
    conf_mat(truth = class, estimate= .pred_class)
```


Logistic Regression


```{r}

set.seed(123)

health1 <- health1 %>%
mutate(DEATH_EVENT = relevel(factor(DEATH_EVENT), ref= '0'))

health_cv13 <- vfold_cv(health1, v = 13)

logistic_spec <- logistic_reg() %>%
  set_engine('glm') %>%
  set_mode('classification')

logistic_rec <- recipe(DEATH_EVENT ~ ., data = health1) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors())

log_wf <- workflow() %>%
  add_recipe(logistic_rec)%>%
  add_model(logistic_spec)

log_fit <- fit(log_wf, data = health1)


log_fit %>% tidy() %>%
  mutate(OR.conf.low = exp(estimate- 1.96*std.error),OR.conf.high = exp(estimate + 1.96*std.error)) %>%
  mutate(OR = exp(estimate))
```

Predictions on the Logistic Model

```{r}
predict(log_fit, new_data = health1, type = "prob")

predict(log_fit, new_data = health1, type = "class")
```


```{r}
logistic_output <-  health1 %>%
  bind_cols(predict(log_fit, new_data = health1, type = 'prob')) 

# do we want to change the threshold?
logistic_output <- logistic_output %>%
  mutate(.pred_class = make_two_class_pred(`.pred_1`, levels(DEATH_EVENT), threshold = .5)) 

logistic_output %>%
  ggplot(aes(x = DEATH_EVENT, y = .pred_1)) +
  geom_boxplot() + 
  geom_hline(yintercept = 0.5, color='red') +  # try changing threshold
  labs(y = 'Predicted Probability of Outcome', x = 'Observed Outcome') +
  theme_classic()
```


```{r}
logistic_output %>%
  conf_mat(truth = DEATH_EVENT, estimate = .pred_class)

log_metrics <- metric_set(sens, yardstick::spec, accuracy) 

logistic_output %>% 
  log_metrics(estimate = .pred_class, truth = DEATH_EVENT, event_level = "second")
```

```{r}
logistic_roc <- logistic_output %>% 
    roc_curve(DEATH_EVENT, .pred_1, event_level = "second")

autoplot(logistic_roc) + theme_classic()
```




```{r}
log_cv_fit <- fit_resamples(
    log_wf, 
    resamples = health_cv13,
    metrics = metric_set(sens, yardstick::spec, accuracy, roc_auc),
    control = control_resamples(save_pred = TRUE, event_level = 'second'))  

collect_metrics(log_cv_fit) 
```

